{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e09f34a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/agent/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 10 threads.\n",
      "NumExpr defaulting to 10 threads.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.finetuning import SentenceTransformersFinetuneEngine\n",
    "from utils.utils_embedding import load_qa_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3496aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to = \"codice\"\n",
    "dataset_name = \"QA/\" + path_to + \"/gpt-35-turbo_dataset.json\"\n",
    "model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "model_output_path_name = f\"finetuned-sentence-transformers/\" + path_to + f\"/finetuned-paraphrase-multilingual-MiniLM-L12-v2_{dataset_name.split('/')[-1].split('.')[0]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f63b6477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_finetuning(dataset_name, model_name, model_output_path_name):\n",
    "    \"\"\" \n",
    "    Finetune the Sentence Transformers model on the specified dataset.\n",
    "    \"\"\"\n",
    "    qa_dataset = load_qa_dataset(dataset_name)\n",
    "    finetune_engine = SentenceTransformersFinetuneEngine(\n",
    "                                                        qa_dataset,\n",
    "                                                        model_id=model_name,\n",
    "                                                        model_output_path=model_output_path_name,\n",
    "                                                        val_dataset=qa_dataset,\n",
    "                                                        )\n",
    "    finetune_engine.finetune()\n",
    "    finetuned_embed_model = finetune_engine.get_finetuned_model()\n",
    "    return finetuned_embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cd1c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-23 17:04:11.828\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.utils_embedding\u001b[0m:\u001b[36mload_qa_dataset\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mLoading dataset from QA/codice/gpt-35-turbo_dataset.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/agent/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='87' max='142' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 87/142 00:42 < 00:27, 2.00 it/s, Epoch 1.21/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Cosine Accuracy@1</th>\n",
       "      <th>Cosine Accuracy@3</th>\n",
       "      <th>Cosine Accuracy@5</th>\n",
       "      <th>Cosine Accuracy@10</th>\n",
       "      <th>Cosine Precision@1</th>\n",
       "      <th>Cosine Precision@3</th>\n",
       "      <th>Cosine Precision@5</th>\n",
       "      <th>Cosine Precision@10</th>\n",
       "      <th>Cosine Recall@1</th>\n",
       "      <th>Cosine Recall@3</th>\n",
       "      <th>Cosine Recall@5</th>\n",
       "      <th>Cosine Recall@10</th>\n",
       "      <th>Cosine Ndcg@10</th>\n",
       "      <th>Cosine Mrr@10</th>\n",
       "      <th>Cosine Map@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.423898</td>\n",
       "      <td>0.546230</td>\n",
       "      <td>0.591750</td>\n",
       "      <td>0.657183</td>\n",
       "      <td>0.423898</td>\n",
       "      <td>0.182077</td>\n",
       "      <td>0.118350</td>\n",
       "      <td>0.065718</td>\n",
       "      <td>0.423898</td>\n",
       "      <td>0.546230</td>\n",
       "      <td>0.591750</td>\n",
       "      <td>0.657183</td>\n",
       "      <td>0.533868</td>\n",
       "      <td>0.495284</td>\n",
       "      <td>0.502831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.435277</td>\n",
       "      <td>0.561878</td>\n",
       "      <td>0.610242</td>\n",
       "      <td>0.665718</td>\n",
       "      <td>0.435277</td>\n",
       "      <td>0.187293</td>\n",
       "      <td>0.122048</td>\n",
       "      <td>0.066572</td>\n",
       "      <td>0.435277</td>\n",
       "      <td>0.561878</td>\n",
       "      <td>0.610242</td>\n",
       "      <td>0.665718</td>\n",
       "      <td>0.548500</td>\n",
       "      <td>0.511184</td>\n",
       "      <td>0.519903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:Information Retrieval Evaluation of the model on the  dataset in epoch 0.704225352112676 after 50 steps:\n",
      "Information Retrieval Evaluation of the model on the  dataset in epoch 0.704225352112676 after 50 steps:\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:Queries: 703\n",
      "Queries: 703\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:Corpus: 703\n",
      "\n",
      "Corpus: 703\n",
      "\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:Score-Function: cosine\n",
      "Score-Function: cosine\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:Accuracy@1: 42.39%\n",
      "Accuracy@1: 42.39%\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:Accuracy@3: 54.62%\n",
      "Accuracy@3: 54.62%\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:Accuracy@5: 59.17%\n",
      "Accuracy@5: 59.17%\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:Accuracy@10: 65.72%\n",
      "Accuracy@10: 65.72%\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:Precision@1: 42.39%\n",
      "Precision@1: 42.39%\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:Precision@3: 18.21%\n",
      "Precision@3: 18.21%\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:Precision@5: 11.83%\n",
      "Precision@5: 11.83%\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:Precision@10: 6.57%\n",
      "Precision@10: 6.57%\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:Recall@1: 42.39%\n",
      "Recall@1: 42.39%\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:Recall@3: 54.62%\n",
      "Recall@3: 54.62%\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:Recall@5: 59.17%\n",
      "Recall@5: 59.17%\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:Recall@10: 65.72%\n",
      "Recall@10: 65.72%\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:MRR@10: 0.4953\n",
      "MRR@10: 0.4953\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:NDCG@10: 0.5339\n",
      "NDCG@10: 0.5339\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:MAP@100: 0.5028\n",
      "MAP@100: 0.5028\n",
      "INFO:sentence_transformers.SentenceTransformer:Save model to finetuned-sentence-transformers/codice/finetuned-paraphrase-multilingual-MiniLM-L12-v2_gpt-35-turbo_dataset\n",
      "Save model to finetuned-sentence-transformers/codice/finetuned-paraphrase-multilingual-MiniLM-L12-v2_gpt-35-turbo_dataset\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:Information Retrieval Evaluation of the model on the  dataset in epoch 1.0 after 71 steps:\n",
      "Information Retrieval Evaluation of the model on the  dataset in epoch 1.0 after 71 steps:\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:Queries: 703\n",
      "Queries: 703\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:Corpus: 703\n",
      "\n",
      "Corpus: 703\n",
      "\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:Score-Function: cosine\n",
      "Score-Function: cosine\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:Accuracy@1: 43.53%\n",
      "Accuracy@1: 43.53%\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:Accuracy@3: 56.19%\n",
      "Accuracy@3: 56.19%\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:Accuracy@5: 61.02%\n",
      "Accuracy@5: 61.02%\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:Accuracy@10: 66.57%\n",
      "Accuracy@10: 66.57%\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:Precision@1: 43.53%\n",
      "Precision@1: 43.53%\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:Precision@3: 18.73%\n",
      "Precision@3: 18.73%\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:Precision@5: 12.20%\n",
      "Precision@5: 12.20%\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:Precision@10: 6.66%\n",
      "Precision@10: 6.66%\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:Recall@1: 43.53%\n",
      "Recall@1: 43.53%\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:Recall@3: 56.19%\n",
      "Recall@3: 56.19%\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:Recall@5: 61.02%\n",
      "Recall@5: 61.02%\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:Recall@10: 66.57%\n",
      "Recall@10: 66.57%\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:MRR@10: 0.5112\n",
      "MRR@10: 0.5112\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:NDCG@10: 0.5485\n",
      "NDCG@10: 0.5485\n",
      "INFO:sentence_transformers.evaluation.InformationRetrievalEvaluator:MAP@100: 0.5199\n",
      "MAP@100: 0.5199\n",
      "INFO:sentence_transformers.SentenceTransformer:Save model to finetuned-sentence-transformers/codice/finetuned-paraphrase-multilingual-MiniLM-L12-v2_gpt-35-turbo_dataset\n",
      "Save model to finetuned-sentence-transformers/codice/finetuned-paraphrase-multilingual-MiniLM-L12-v2_gpt-35-turbo_dataset\n"
     ]
    }
   ],
   "source": [
    "finetuned_embed_model = run_finetuning(dataset_name, model_name, model_output_path_name)\n",
    "finetuned_embed_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
